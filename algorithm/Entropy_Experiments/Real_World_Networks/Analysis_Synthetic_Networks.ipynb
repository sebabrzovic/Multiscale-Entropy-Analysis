{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing Figures 2, 3, and A3: Multiscale Entropy in Real-World Networks\n",
    "\n",
    "This notebook reproduces **Figures 2 and 3** from Section 4.2 and **Figure A3** from the Appendix, which analyze multiscale entropy behavior across real-world networks from six domains: Biological, Social, Economic, Technological, Transportation, and Informational.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We apply the multiscale entropy framework to 439 undirected networks from the ICON dataset, stratified by:\n",
    "\n",
    "1. **Domain**: Biological, Social, Economic, Technological, Transportation, Informational\n",
    "2. **Size**: Small (0-200 nodes), Medium (200-600 nodes), Large (600+ nodes)\n",
    "\n",
    "The analysis reveals three distinct entropy trajectory patterns:\n",
    "- **Stable behavior**: Biological, Transport, and Informational networks\n",
    "- **Increasing behavior**: Economic and Technological networks  \n",
    "- **Hybrid behavior**: Social networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.5.3 in /home/sebabrzovic/.local/lib/python3.8/site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.20.3; python_version < \"3.10\" in /home/sebabrzovic/.local/lib/python3.8/site-packages (from pandas==1.5.3) (1.24.4)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas==1.5.3\n",
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1441/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "##get current file directory\n",
    "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "##get the parent directory\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "# Add the src directory to sys.path\n",
    "src_dir = os.path.join(parent_dir, '../')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from algorithm.calculo_entropia import *\n",
    "import algorithm.calculo_entropia\n",
    "\n",
    "from algorithm.coarsening_utils import *\n",
    "import algorithm.graph_utils\n",
    "import algorithm.coarsening_utils as cu\n",
    "from algorithm.coarsening_utils import plot_coarsening_vertical\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import networkx as nx\n",
    "import pygsp as gsp\n",
    "from pygsp import graphs\n",
    "gsp.plotting.BACKEND = 'matplotlib'\n",
    "\n",
    "import pickle\n",
    "\n",
    "def save_graphs(graph_dict, filename):\n",
    "    \"\"\"\n",
    "    Save the dictionary of graphs to a file.\n",
    "    \"\"\"\n",
    "    # Convert PyGSP graphs to NetworkX graphs for easier serialization\n",
    "    nx_graph_dict = {\n",
    "        size: [nx.from_scipy_sparse_array(g.W) for g in graphs]\n",
    "        for size, graphs in graph_dict.items()\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(nx_graph_dict, f)\n",
    "    print(f\"Graphs saved to {filename}\")\n",
    "\n",
    "def load_graphs(filename):\n",
    "    \"\"\"\n",
    "    Load the dictionary of graphs from a file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        nx_graph_dict = pickle.load(f)\n",
    "    print(f\"Graphs loaded from {filename}\")\n",
    "    return nx_graph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  \n",
    "# load the data \n",
    "infile = open('./CommunityFitNet_updated.pickle','rb')  \n",
    "df = pickle.load(infile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read edge lists for all networks\n",
    "df_edgelists = df['edges_id'] # column 'edges_id' in dataframe df includes the edge list \n",
    "                              # for each network \n",
    " \n",
    "# extract the edge list for the first network \n",
    "edges_orig = df_edgelists.iloc[0] # a numpy array of edge list for original graph \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading and Preprocessing\n",
    "\n",
    "We load the undirected networks from the ICON dataset and prepare them for multiscale analysis. The dataset has been pre-filtered to contain only undirected graphs compatible with the spectral reduction algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Multiscale Reduction and Entropy Calculation\n",
    "\n",
    "For each network, we apply spectral coarsening at reduction levels of 100%, 80%, 60%, 40%, and 20% of the original size. At each level, we:\n",
    "\n",
    "1. Apply the spectral coarsening algorithm with K=10 eigenvalues preserved\n",
    "2. Convert the reduced graph to NetworkX format\n",
    "3. Compute compression-based entropy using arithmetic encoding\n",
    "4. Normalize against 10 Erdős-Rényi random graphs with matched size and density\n",
    "\n",
    "This process generates a comprehensive JSON file (`graph_families_analysis.json`) containing:\n",
    "- Graph metadata (domain, subdomain, node/edge types)\n",
    "- Structural metrics at each reduction level (nodes, edges, average degree)\n",
    "- Entropy measurements (raw, random baseline, normalized)\n",
    "\n",
    "**Note**: This step is computationally intensive and may take several hours depending on the number of networks. Progress is tracked using tqdm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from pygsp import graphs\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "import pandas as pd\n",
    "\n",
    "# Load the graph processing functions\n",
    "\n",
    "import algorithm.calculo_entropia as ce\n",
    "\n",
    "def create_networkx_graph(row):\n",
    "    \"\"\"Create a NetworkX graph from a row of the dataset.\"\"\"\n",
    "    is_directed = 'Directed' in row['graphProperties']\n",
    "    G = nx.DiGraph() if is_directed else nx.Graph()\n",
    "    \n",
    "    nodes = np.array(row['nodes_id'])\n",
    "    edges = np.array(row['edges_id'])\n",
    "    \n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def calculate_graph_metrics(G_nx):\n",
    "    \"\"\"Calculate basic metrics for a graph.\"\"\"\n",
    "    return {\n",
    "        'number_nodes': G_nx.number_of_nodes(),\n",
    "        'number_edges': G_nx.number_of_edges(),\n",
    "        'ave_degree': 2 * G_nx.number_of_edges() / G_nx.number_of_nodes()\n",
    "    }\n",
    "\n",
    "def calculate_entropies(G_nx):\n",
    "    \"\"\"Calculate entropy metrics using arithmetic encoding method.\"\"\"\n",
    "    # Get entropy metrics directly using the helper function\n",
    "    entropy_metrics = cu.get_entropy_metadata_aritmethicEncoding(G_nx)\n",
    "    \n",
    "    return {\n",
    "        'entropy_arithmetic': {\n",
    "            'graph': entropy_metrics['Grafo'],\n",
    "            'random': entropy_metrics['Grafo_r'],\n",
    "            'normalized': entropy_metrics['Entropy Normalizado']\n",
    "        }\n",
    "    }\n",
    "\n",
    "def process_graph_at_reduction(G_nx, reduction_percent):\n",
    "    \"\"\"Process a graph at a specific reduction percentage.\"\"\"\n",
    "    # Convert to PyGSP format\n",
    "    W = nx.to_scipy_sparse_array(G_nx)\n",
    "    G = graphs.Graph(W)\n",
    "    \n",
    "    # Calculate reduction ratio\n",
    "    r = 1 - (reduction_percent/100)\n",
    "    \n",
    "    if reduction_percent == 100:\n",
    "        # Original graph, no reduction needed\n",
    "        G_reduced = G_nx\n",
    "    else:\n",
    "        # Perform coarsening\n",
    "        try:\n",
    "            C, Gc, Call, Gall = coarsen(G, K=10, r=r)\n",
    "            G_reduced = nx.from_scipy_sparse_array(Gc.W)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in coarsening at {reduction_percent}%: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    # Calculate metrics and entropies\n",
    "    metrics = calculate_graph_metrics(G_reduced)\n",
    "    entropies = calculate_entropies(G_reduced)\n",
    "    \n",
    "    # Combine all information\n",
    "    return {\n",
    "        'graph_portion': reduction_percent,\n",
    "        **metrics,\n",
    "        **entropies\n",
    "    }\n",
    "\n",
    "\n",
    "def create_graph_family_json(df):\n",
    "    \"\"\"Create the complete JSON structure for all graph families.\"\"\"\n",
    "    result = defaultdict(lambda: defaultdict(dict))\n",
    "    \n",
    "    # Process each row in the dataframe\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        family = row['networkDomain']\n",
    "        if not isinstance(family, str):\n",
    "            continue\n",
    "            \n",
    "        graph_data = {\n",
    "            'Name': row['network_name'],\n",
    "            'Subdomain': row['subDomain'],\n",
    "            'Node_Type': row['nodeType'],\n",
    "            'Edge_Type': row['edgeType'],\n",
    "            'reductions': {}\n",
    "        }\n",
    "        \n",
    "        # Create NetworkX graph\n",
    "        G_nx = create_networkx_graph(row)\n",
    "        \n",
    "        # Process each reduction percentage\n",
    "        for reduction in [100, 80, 60, 40, 20]:\n",
    "            reduction_data = process_graph_at_reduction(G_nx, reduction)\n",
    "            if reduction_data:\n",
    "                graph_data['reductions'][str(reduction)] = reduction_data\n",
    "        \n",
    "        # Add to result\n",
    "        result[family][row['network_name']] = graph_data\n",
    "    \n",
    "    return dict(result)\n",
    "\n",
    "# Load the data\n",
    "with open('undirected_networks.pkl', 'rb') as f:\n",
    "    real_graphs = pickle.load(f)\n",
    "\n",
    "# Process all graphs and create JSON\n",
    "graph_families = create_graph_family_json(real_graphs)\n",
    "\n",
    "# Save to JSON file\n",
    "with open('graph_families_analysis.json', 'w') as f:\n",
    "    json.dump(graph_families, f, indent=2)\n",
    "\n",
    "# Example of accessing the data\n",
    "print(\"Available network domains:\", list(graph_families.keys()))\n",
    "for domain in graph_families:\n",
    "    print(f\"\\n{domain} networks:\", list(graph_families[domain].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Case Study Visualization - Figure A3\n",
    "\n",
    "To illustrate the diversity of entropy behaviors, we select 6 representative networks—one from each domain—and visualize their coarsening process across all reduction levels. This produces **Figure A3** in the Appendix.\n",
    "\n",
    "### Selection Criteria\n",
    "\n",
    "Networks are selected to maximize domain diversity while choosing medium-sized examples (median node count within each domain) for optimal visualization. If fewer than 6 domains are available, we select from different subdomains to ensure structural diversity.\n",
    "\n",
    "### Visualization Details\n",
    "\n",
    "For each network, we generate a horizontal panel showing:\n",
    "- The graph structure at each reduction level (100%, 80%, 60%, 40%, 20%)\n",
    "- Node and edge counts at each scale\n",
    "- Visual preservation of key structural features\n",
    "\n",
    "The visualization uses spring layout for node positioning and color-codes nodes by their contraction sets to show how the coarsening algorithm groups vertices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  \n",
    "# load the data \n",
    "infile = open('./undirected_networks.pkl','rb')  \n",
    "df = pickle.load(infile) \n",
    "\n",
    "# read edge lists for all networks\n",
    "df_edgelists = df['edges_id'] # column 'edges_id' in dataframe df includes the edge list \n",
    "                              # for each network \n",
    " \n",
    "# extract the edge list for the first network \n",
    "edges_orig = df_edgelists.iloc[0] # a numpy array of edge list for original graph \n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# After loading your df:\n",
    "print(df.head())  # See first few rows\n",
    "print(df.columns)  # See what columns we have\n",
    "print(df.info()) \n",
    "print(len(df))  # Number of rows in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple analysis of 6 networks from different families\n",
    "\n",
    "import numpy as np\n",
    "import pygsp as gsp\n",
    "import networkx as nx\n",
    "\n",
    "# Parameters\n",
    "reduction_levels = [0.2, 0.4, 0.6, 0.8]\n",
    "method = \"variation_neighborhood\"\n",
    "K = 5\n",
    "\n",
    "# First, let's see what families we have\n",
    "print(\"=== Available Network Families ===\")\n",
    "families = df.groupby(['networkDomain', 'subDomain']).agg({\n",
    "    'network_index': 'count',\n",
    "    'number_nodes': ['min', 'max'],\n",
    "    'number_edges': ['min', 'max']\n",
    "}).round(0)\n",
    "print(families)\n",
    "print()\n",
    "\n",
    "# Select 6 networks - prioritize getting one from each DOMAIN first\n",
    "print(\"=== Selecting Networks by Domain ===\")\n",
    "domains = df['networkDomain'].unique()\n",
    "print(f\"Available domains: {domains}\")\n",
    "\n",
    "selected_networks = []\n",
    "used_domains = set()\n",
    "\n",
    "# First pass: get one network from each domain\n",
    "for domain in domains:\n",
    "    domain_networks = df[df['networkDomain'] == domain]\n",
    "    if len(domain_networks) > 0:\n",
    "        # Pick a medium-sized network from this domain\n",
    "        sorted_by_size = domain_networks.sort_values('number_nodes')\n",
    "        median_idx = len(sorted_by_size) // 2\n",
    "        selected_network = sorted_by_size.iloc[median_idx]\n",
    "        selected_networks.append(selected_network)\n",
    "        used_domains.add(domain)\n",
    "        print(f\"Selected from {domain}: {selected_network['title'][:50]}...\")\n",
    "        \n",
    "        if len(selected_networks) >= 6:\n",
    "            break\n",
    "\n",
    "# Second pass: if we still need more networks, get from different subdomains\n",
    "if len(selected_networks) < 6:\n",
    "    print(\"\\nNeed more networks, selecting from different subdomains...\")\n",
    "    \n",
    "    # Get all unique domain-subdomain combinations we haven't used\n",
    "    used_combinations = set()\n",
    "    for net in selected_networks:\n",
    "        used_combinations.add((net['networkDomain'], net['subDomain']))\n",
    "    \n",
    "    remaining_combinations = []\n",
    "    for _, row in df.iterrows():\n",
    "        combo = (row['networkDomain'], row['subDomain'])\n",
    "        if combo not in used_combinations:\n",
    "            remaining_combinations.append(row)\n",
    "    \n",
    "    # Sort by domain diversity, then by size\n",
    "    remaining_df = pd.DataFrame(remaining_combinations)\n",
    "    if len(remaining_df) > 0:\n",
    "        for _, network in remaining_df.iterrows():\n",
    "            selected_networks.append(network)\n",
    "            print(f\"Added from {network['networkDomain']}-{network['subDomain']}: {network['title'][:50]}...\")\n",
    "            if len(selected_networks) >= 6:\n",
    "                break\n",
    "\n",
    "# Print information for selected networks\n",
    "print(\"=== Selected Networks Information ===\")\n",
    "for i, network in enumerate(selected_networks[:6], 1):\n",
    "    print(f\"Network {i}:\")\n",
    "    print(f\"  Title: {network['title']}\")\n",
    "    print(f\"  Description: {network['description']}\")\n",
    "    print(f\"  Domain: {network['networkDomain']}\")\n",
    "    print(f\"  Subdomain: {network['subDomain']}\")\n",
    "    print(f\"  Nodes: {network['number_nodes']}\")\n",
    "    print(f\"  Edges: {network['number_edges']}\")\n",
    "    print(f\"  Average Degree: {network['ave_degree']:.2f}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Function to create PyGSP graph from edges\n",
    "def create_graph_from_edges(edges_array):\n",
    "    nx_graph = nx.Graph()\n",
    "    nx_graph.add_edges_from(edges_array)\n",
    "    adj_matrix = nx.adjacency_matrix(nx_graph)\n",
    "    G = gsp.graphs.Graph(adj_matrix)\n",
    "    # Add coordinates for visualization\n",
    "    pos = nx.spring_layout(nx_graph, seed=42)\n",
    "    coords = np.array([pos.get(i, [0, 0]) for i in range(G.N)])\n",
    "    G.set_coordinates(coords)\n",
    "    return G\n",
    "\n",
    "# Visualize each network's coarsening\n",
    "print(\"\\n=== Visualizing Network Coarsening ===\")\n",
    "for i, network in enumerate(selected_networks[:6], 1):\n",
    "    print(f\"\\nProcessing Network {i}: {network['title'][:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get edges for this network\n",
    "        edges_orig = df[df['network_index'] == network['network_index']]['edges_id'].iloc[0]\n",
    "        \n",
    "        # Create PyGSP graph\n",
    "        G = create_graph_from_edges(edges_orig)\n",
    "        print(f\"Created graph with {G.N} nodes, {G.Ne} edges\")\n",
    "        \n",
    "        # Create visualization\n",
    "        title = f\"Network {i}: {network['title'][:40]}{'...' if len(network['title']) > 40 else ''}\"\n",
    "        fig = plot_reduction_levels_horizontal(\n",
    "            G,\n",
    "            reduction_levels=reduction_levels,\n",
    "            method=method,\n",
    "            K=K,\n",
    "            size=3,\n",
    "            alpha=0.7,\n",
    "            title=title\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with network {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nDone! ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Cluster Analysis - Figure 4\n",
    "\n",
    "To quantitatively validate the three distinct entropy trajectory patterns observed in Figures 2 and 3, we perform unsupervised clustering on the multiscale entropy profiles. This analysis produces **Figure 4** from Section 4.3.\n",
    "\n",
    "### Clustering Methodology\n",
    "\n",
    "Each network is represented as a 5-dimensional feature vector containing normalized compression entropy values at all reduction levels: [100%, 80%, 60%, 40%, 20%].\n",
    "\n",
    "We apply K-means clustering with k=3 to identify the three behavioral regimes:\n",
    "\n",
    "1. **Cluster 1 (Hybrid)**: Social networks showing stable entropy under moderate reduction, with sharp increases at aggressive compression\n",
    "2. **Cluster 2 (Increasing)**: Economic and Technological networks exhibiting early structural degradation\n",
    "3. **Cluster 3 (Stable)**: Biological, Transportation, and Informational networks maintaining consistent entropy across scales\n",
    "\n",
    "### Dimensionality Reduction\n",
    "\n",
    "To visualize the 5-dimensional entropy space, we apply Principal Component Analysis (PCA) to project data into 2D while preserving maximum variance. The resulting scatter plot shows:\n",
    "\n",
    "- Each point represents a network, colored by domain\n",
    "- Red \"X\" markers indicate cluster centroids\n",
    "- Dashed gray circles illustrate cluster boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Cargar los datos\n",
    "with open(\"graph_families_analysis.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extraer los valores de entropía normalizada de length compression\n",
    "graphs = []  # Lista de nombres de grafos\n",
    "features = []  # Lista para almacenar los vectores de 5 dimensiones\n",
    "families = []\n",
    "\n",
    "for family, graphs_data in data.items():\n",
    "    for graph_name, graph_info in graphs_data.items():\n",
    "        if \"reductions\" in graph_info:\n",
    "            reductions = graph_info[\"reductions\"]\n",
    "            if all(str(p) in reductions for p in [\"100\", \"80\", \"60\", \"40\", \"20\"]):\n",
    "                entropy_values = [\n",
    "                    reductions[str(p)][\"entropy_arithmetic\"][\"normalized\"]\n",
    "                    for p in [100, 80, 60, 40, 20]\n",
    "                ]\n",
    "                features.append(entropy_values)\n",
    "                graphs.append(graph_name)\n",
    "                families.append(family)\n",
    "\n",
    "# Convertir a numpy array\n",
    "X = np.array(features)\n",
    "\n",
    "# Estandarizar los datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Aplicar KMeans con 3 clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "kmeans.fit(X_scaled)\n",
    "labels = kmeans.labels_\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "# Visualizar los clusters usando PCA para reducción a 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "centers_pca = pca.transform(cluster_centers)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=families, palette=\"tab10\", s=100, alpha=0.8)\n",
    "plt.scatter(centers_pca[:, 0], centers_pca[:, 1], c=\"red\", marker=\"x\", s=200, label=\"Cluster Center\")\n",
    "\n",
    "# Dibujar círculos alrededor de los clusters\n",
    "for center in centers_pca:\n",
    "    plt.gca().add_patch(plt.Circle(center, 0.5, color='gray', fill=False, linestyle='dashed'))\n",
    "\n",
    "plt.title(\"Graph Clustering by Length Compression Normalized Entropy (KMeans, k=3)\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.legend(title=\"Family Domain\")\n",
    "plt.show()\n",
    "\n",
    "# Agrupar datos por cluster\n",
    "cluster_composition = defaultdict(lambda: defaultdict(int))\n",
    "for i, (family, graph_name) in enumerate(zip(families, graphs)):\n",
    "    cluster_composition[labels[i]][family] += 1\n",
    "\n",
    "# Mostrar la composición de los clusters\n",
    "print(\"Composición de los clusters:\\n\")\n",
    "for cluster, family_counts in sorted(cluster_composition.items()):\n",
    "    print(f\"Cluster {cluster}:\")\n",
    "    for family, count in family_counts.items():\n",
    "        print(f\"{family}: {count} grafos\")\n",
    "    print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
